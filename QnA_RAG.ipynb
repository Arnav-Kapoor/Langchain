{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285384a6-4be6-43c1-bbb2-201d92ecf90d",
   "metadata": {},
   "source": [
    "##### QnA chatcot using RAG. context is standford lectures. this is a take home assignment for an application(Ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8492115d-27ed-4237-82dc-271bc0c2f2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80aff92-00b5-4fa8-92a3-14dc2aaff65c",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5226de6-9437-436b-ba8a-2683fde2d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader1=WebBaseLoader(web_paths=(\"https://stanford-cs324.github.io/winter2022/lectures/introduction/\",),\n",
    "                    bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"main-content-wrap\")))) \n",
    "loader2=WebBaseLoader(web_paths=(\"https://stanford-cs324.github.io/winter2022/lectures/legality/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"main-content-wrap\"))))\n",
    "loader3=WebBaseLoader(web_paths=(\"https://stanford-cs324.github.io/winter2022/lectures/modeling/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"main_content-wrap\"))))\n",
    "loader4=WebBaseLoader(web_paths=(\"https://stanford-cs324.github.io/winter2022/lectures/training/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"main-content-wrap\"))))\n",
    "### we are using 4 lectures: intro, legality, modeling and training. \n",
    "### since all the content was inside the main class \"main-content-wrap\" we used that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2aea361-9e32-4238-9dc5-c9e016b41cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders=[loader1,loader2,loader3,loader4] #array of loaders to work upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d211aa3-92e7-4348-bcc3-2cf1fef59ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load()) #loading documents together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da51bc-0ad4-4ec5-b7c3-83b3713cd729",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2683f719-40c3-4a44-bd2b-522c39de258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ece27eb7-3f41-46be-9237-fa5b221bdd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split=splitter.split_documents(docs) # splitting documents into chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15b542be-2cc2-4c5d-8ba8-ca9cfc71a174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37e16600-52c2-477c-a9a6-9661760efafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LecturesIntroduction \\\\[\\\\newcommand{\\\\sV}{\\\\mathcal{V}} \\\\newcommand{\\\\nl}[1]{\\\\textsf{#1}} \\\\newcommand{\\\\generate}[1]{\\\\stackrel{#1}{\\\\rightsquigarrow}}\\\\]Welcome to CS324! This is a new course on understanding and developing large language models.What is a language model?A brief historyWhy does this course exist?Structure of this course  What is a language model?The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\\\(\\\\sV\\\\) of a set of tokens. A language model \\\\(p\\\\) assigns each sequence of tokens \\\\(x_1, \\\\dots, x_L \\\\in \\\\sV\\\\) a probability (a number between 0 and 1):\\\\[p(x_1, \\\\dots, x_L).\\\\]The probability intuitively tells us how “good” a sequence of tokens is. For example, if the vocabulary is \\\\(\\\\sV = \\\\{ \\\\nl{ate}, \\\\nl{ball}, \\\\nl{cheese}, \\\\nl{mouse}, \\\\nl{the} \\\\}\\\\), the language model might assign (demo):\\\\[p(\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}) = 0.02,\\\\] \\\\[p(\\\\nl{the}, \\\\nl{cheese}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{mouse}) = 0.01,\\\\] \\\\[p(\\\\nl{mouse}, \\\\nl{the}, \\\\nl{the}, \\\\nl{cheese}, \\\\nl{ate}) = 0.0001.\\\\]Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit) linguistic abilities and world knowledge.For example, the LM should assign \\\\(\\\\nl{mouse the the cheese ate}\\\\) a very low probability implicitly because it’s ungrammatical (syntactic knowledge). The LM should assign \\\\(\\\\nl{the mouse ate the cheese}\\\\) higher probability than \\\\(\\\\nl{the cheese ate the mouse}\\\\) implicitly because of world knowledge: both sentences are the same syntactically, but they differ in semantic plausibility.Generation. As defined, a language model \\\\(p\\\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\\\(x_{1:L}\\\\) from the language model \\\\(p\\\\) with probability equal to \\\\(p(x_{1:L})\\\\), denoted:\\\\[x_{1:L} \\\\sim p.\\\\]How to do this computationally efficiently depends on the form of the language model \\\\(p\\\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an “average” sequence but something closer to the “best” sequence.  Autoregressive language modelsA common way to write the joint distribution \\\\(p(x_{1:L})\\\\) of a sequence \\\\(x_{1:L}\\\\) is using the chain rule of probability:\\\\[p(x_{1:L}) = p(x_1) p(x_2 \\\\mid x_1) p(x_3 \\\\mid x_1, x_2) \\\\cdots p(x_L \\\\mid x_{1:L-1}) = \\\\prod_{i=1}^L p(x_i \\\\mid x_{1:i-1}).\\\\]For example (demo):\\\\[\\\\begin{align*} p(\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}) = \\\\, & p(\\\\nl{the}) \\\\\\\\ & p(\\\\nl{mouse} \\\\mid \\\\nl{the}) \\\\\\\\ & p(\\\\nl{ate} \\\\mid \\\\nl{the}, \\\\nl{mouse}) \\\\\\\\ & p(\\\\nl{the} \\\\mid \\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}) \\\\\\\\ & p(\\\\nl{cheese} \\\\mid \\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}). \\\\end{align*}\\\\]In particular, \\\\(p(x_i \\\\mid x_{1:i-1})\\\\) is a conditional probability distribution of the next token \\\\(x_i\\\\) given the previous tokens \\\\(x_{1:i-1}\\\\).Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\\\(p(x_i \\\\mid x_{1:i-1})\\\\) can be computed efficiently (e.g., using a feedforward neural network).Generation. Now to generate an entire sequence \\\\(x_{1:L}\\\\) from an autoregressive language model \\\\(p\\\\), we sample one token at a time given the tokens generated so far:\\\\[\\\\text{for } i = 1, \\\\dots, L: \\\\\\\\ \\\\hspace{1in} x_i \\\\sim p(x_i \\\\mid x_{1:i-1})^{1/T},\\\\]where \\\\(T \\\\ge 0\\\\) is a temperature parameter that controls how much randomness we want from the language model:\\\\(T = 0\\\\): deterministically choose the most probable token \\\\(x_i\\\\) at each position \\\\(i\\\\)\\\\(T = 1\\\\): sample “normally” from the pure language model\\\\(T = \\\\infty\\\\): sample from a uniform distribution over the entire vocabulary \\\\(\\\\sV\\\\)However, if we just raise the probabilities to the power \\\\(1/T\\\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\\\(p_T(x_i \\\\mid x_{1:i-1}) \\\\propto p(x_i \\\\mid x_{1:i-1})^{1/T}\\\\) the annealed conditional probability distribution. For example:\\\\[p(\\\\nl{cheese}) = 0.4, \\\\quad\\\\quad\\\\quad p(\\\\nl{mouse}) = 0.6\\\\] \\\\[p_{T=0.5}(\\\\nl{cheese}) = 0.31, \\\\quad\\\\quad\\\\quad p_{T=0.5}(\\\\nl{mouse}) = 0.69\\\\] \\\\[p_{T=0.2}(\\\\nl{cheese}) = 0.12, \\\\quad\\\\quad\\\\quad p_{T=0.2}(\\\\nl{mouse}) = 0.88\\\\] \\\\[p_{T=0}(\\\\nl{cheese}) = 0, \\\\quad\\\\quad\\\\quad p_{T=0}(\\\\nl{mouse}) = 1\\\\]Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing.Technical note: sampling iteratively with a temperature \\\\(T\\\\) parameter applied to each conditional distribution \\\\(p(x_i \\\\mid x_{1:i-1})^{1/T}\\\\) is not equivalent (except when \\\\(T = 1\\\\)) to sampling from the annealed distribution over length \\\\(L\\\\) sequences.Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\\\(x_{1:i}\\\\) (called a prompt) and sampling the rest \\\\(x_{i+1:L}\\\\) (called the completion). For example, generating with \\\\(T=0\\\\) produces (demo):\\\\[\\\\underbrace{\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}}_\\\\text{prompt} \\\\generate{T=0} \\\\underbrace{\\\\nl{the}, \\\\nl{cheese}}_\\\\text{completion}.\\\\]If we change the temperature to \\\\(T = 1\\\\), we can get more variety (demo), for example, \\\\(\\\\nl{its house}\\\\) and \\\\(\\\\nl{my homework}\\\\).As we’ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.  SummaryA language model is a probability distribution \\\\(p\\\\) over sequences \\\\(x_{1:L}\\\\).Intuitively, a good language model should have linguistic capabilities and world knowledge.An autoregressive language model allows for efficient generation of a completion \\\\(x_{i+1:L}\\\\) given a prompt \\\\(x_{1:i}\\\\).The temperature can be used to control the amount of variability in generation.  A brief history  Information theory, entropy of English, n-gram modelsInformation theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as\\\\[H(p) = \\\\sum_x p(x) \\\\log \\\\frac{1}{p(x)}.\\\\]The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\\\(x \\\\sim p\\\\) into a bitstring:\\\\[\\\\nl{the mouse ate the cheese} \\\\Rightarrow 0001110101.\\\\]The lower the entropy, the more “structured” the sequence is, and the shorter the code length.Intuitively, \\\\(\\\\log \\\\frac{1}{p(x)}\\\\) is the length of the code used to represent an element \\\\(x\\\\) that occurs with probability \\\\(p(x)\\\\).If \\\\(p(x) = \\\\frac{1}{8}\\\\), we should allocate \\\\(\\\\log_2(8) = 3\\\\) bits (equivalently, \\\\(\\\\log(8) = 2.08\\\\) nats).Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory.Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a “true” distribution \\\\(p\\\\) out there (the existence of this is questionable, but it’s still a useful mathematical abstraction) that can spout out samples of English text \\\\(x \\\\sim p\\\\).Shannon also defined cross entropy:\\\\[H(p, q) = \\\\sum_x p(x) \\\\log \\\\frac{1}{q(x)},\\\\]which measures the expected number of bits (nats) needed to encode a sample \\\\(x \\\\sim p\\\\) using the compression scheme given by the model \\\\(q\\\\) (representing \\\\(x\\\\) with a code of length \\\\(\\\\frac{1}{q(x)}\\\\)).Estimating entropy via language modeling. A crucial property is that the cross entropy \\\\(H(p, q)\\\\) upper bounds the entropy \\\\(H(p)\\\\),\\\\[H(p, q) \\\\ge H(p),\\\\]which means that we can estimate \\\\(H(p, q)\\\\) by constructing a (language) model \\\\(q\\\\) with only samples from the true data distribution \\\\(p\\\\), whereas \\\\(H(p)\\\\) is generally inaccessible if \\\\(p\\\\) is English.So we can get better estimates of the entropy \\\\(H(p)\\\\) by constructing better models \\\\(q\\\\), as measured by \\\\(H(p, q)\\\\).Shannon game (human language model). Shannon first used n-gram models as \\\\(q\\\\) in 1948, but in his 1951 paper Prediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\\\(q\\\\) was provided by a human:\\\\[\\\\nl{the mouse ate my ho_}\\\\]Humans aren’t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses.  N-gram models for downstream applicationsLanguage models became first used in practical applications that required generation of text:speech recognition in the 1970s (input: acoustic signal, output: text), andmachine translation in the 1990s (input: text in a source language, output: text in a target language).Noisy channel model. The dominant paradigm for solving these tasks then was the noisy channel model. Taking speech recognition as an example:We posit that there is some text sampled from some distribution \\\\(p\\\\).This text becomes realized to speech (acoustic signals).Then given the speech, we wish to recover the (most likely) text. This can be done via Bayes rule:\\\\[p(\\\\text{text} \\\\mid \\\\text{speech}) \\\\propto \\\\underbrace{p(\\\\text{text})}_\\\\text{language model} \\\\underbrace{p(\\\\text{speech} \\\\mid \\\\text{text})}_\\\\text{acoustic model}.\\\\]Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).N-gram models. In an n-gram model, the prediction of a token \\\\(x_i\\\\) only depends on the last \\\\(n-1\\\\) characters \\\\(x_{i-(n-1):i-1}\\\\) rather than the full history:\\\\[p(x_i \\\\mid x_{1:i-1}) = p(x_i \\\\mid x_{i-(n-1):i-1}).\\\\]For example, a trigram (\\\\(n=3\\\\)) model would define:\\\\[p(\\\\nl{cheese} \\\\mid \\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}) ='"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd340082-c550-4d40-8fd7-2beb6355d765",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "59664c4d-e14d-41f3-8073-904135d47e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_message=(\"\"\" You are a question-answer assistant.\n",
    "                    Answer the questions based on the context provided.\n",
    "                    Keep the answers as accurate and concise as possible.\n",
    "                    If the answer is not present in the context then say its not available in the context.\n",
    "                    \\n\\n\n",
    "                    {context}.\"\"\")\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([(\"system\",system_message),\n",
    "                                        (\"human\",\"{input}\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d05bbc6a-7ceb-46b3-9489-83f1d8e022dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model=Ollama(model=\"llama3\",num_gpu=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "533b673a-1c4a-4c99-8a76-d20e3ad2870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "stuff_doc_chain=create_stuff_documents_chain(model,prompt) #creating stuff document chain or basically chaining llm and prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d641b2bb-1665-44d0-a85d-377a6178b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_storage=Chroma(embedding_function=OllamaEmbeddings(),persist_directory=\"/vector_store\") #initializing our vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "591967e7-9fca-4284-b6c5-bb9debeb99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryByteStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fe330a99-3e77-450a-b427-c27304c1a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=MultiVectorRetriever(vectorstore=vector_storage, byte_store=InMemoryByteStore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c6e64b5-e7a6-4198-a289-8c60060a85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_splitter=RecursiveCharacterTextSplitter(chunk_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7f5b5a8f-243d-45bb-8de9-ae4e47d7fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs=[]\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    sub_doc=child_splitter.split_documents([doc])\n",
    "    sub_docs.extend(sub_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bfce0afe-8380-4950-9b15-c41228f0ba57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be6220-6e0d-4e08-97ec-4cde9a65dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb587d-e6fb-4ffc-9eb4-f8a9dc2728e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e86dd6-f054-4394-b6a1-dd957a3b9aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
