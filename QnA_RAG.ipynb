{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285384a6-4be6-43c1-bbb2-201d92ecf90d",
   "metadata": {},
   "source": [
    "##### QnA chatcot using RAG. context is standford lectures. this is a take home assignment for an application(Ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8492115d-27ed-4237-82dc-271bc0c2f2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80aff92-00b5-4fa8-92a3-14dc2aaff65c",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5226de6-9437-436b-ba8a-2683fde2d08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader1=WebBaseLoader(web_paths=(\"https://stanford-cs324.github.io/winter2022/lectures/introduction/\",),\n",
    "                    bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"main-content-wrap\")))) \n",
    "loader2=WebBaseLoader(web_paths=(\"https://stanford-cs324.github.io/winter2022/lectures/legality/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"main-content-wrap\"))))\n",
    "loader3=WebBaseLoader(web_paths=(\"https://stanford-cs324.github.io/winter2022/lectures/modeling/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"main_content-wrap\"))))\n",
    "loader4=WebBaseLoader(web_paths=(\"https://stanford-cs324.github.io/winter2022/lectures/training/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"main-content-wrap\"))))\n",
    "### we are using 4 lectures: intro, legality, modeling and training. \n",
    "### since all the content was inside the main class \"main-content-wrap\" we used that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2aea361-9e32-4238-9dc5-c9e016b41cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders=[loader1,loader2,loader3,loader4] #array of loaders to work upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d211aa3-92e7-4348-bcc3-2cf1fef59ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load()) #loading documents together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da51bc-0ad4-4ec5-b7c3-83b3713cd729",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2683f719-40c3-4a44-bd2b-522c39de258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ece27eb7-3f41-46be-9237-fa5b221bdd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split=splitter.split_documents(docs) # splitting documents into chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15b542be-2cc2-4c5d-8ba8-ca9cfc71a174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37e16600-52c2-477c-a9a6-9661760efafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LecturesIntroduction \\\\[\\\\newcommand{\\\\sV}{\\\\mathcal{V}} \\\\newcommand{\\\\nl}[1]{\\\\textsf{#1}} \\\\newcommand{\\\\generate}[1]{\\\\stackrel{#1}{\\\\rightsquigarrow}}\\\\]Welcome to CS324! This is a new course on understanding and developing large language models.What is a language model?A brief historyWhy does this course exist?Structure of this course  What is a language model?The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\\\(\\\\sV\\\\) of a set of tokens. A language model \\\\(p\\\\) assigns each sequence of tokens \\\\(x_1, \\\\dots, x_L \\\\in \\\\sV\\\\) a probability (a number between 0 and 1):\\\\[p(x_1, \\\\dots, x_L).\\\\]The probability intuitively tells us how “good” a sequence of tokens is. For example, if the vocabulary is \\\\(\\\\sV = \\\\{ \\\\nl{ate}, \\\\nl{ball}, \\\\nl{cheese}, \\\\nl{mouse}, \\\\nl{the} \\\\}\\\\), the language model might assign (demo):\\\\[p(\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}) = 0.02,\\\\] \\\\[p(\\\\nl{the}, \\\\nl{cheese}, \\\\nl{ate}, \\\\nl{the},'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd340082-c550-4d40-8fd7-2beb6355d765",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59664c4d-e14d-41f3-8073-904135d47e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_message=(\"\"\" You are a question-answer assistant.\n",
    "                    Answer the questions based on the context provided.\n",
    "                    Keep the answers as accurate and concise as possible.\n",
    "                    If the answer is not present in the context then say its not available in the context.\n",
    "                    \\n\\n\n",
    "                    {context}.\"\"\")\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([(\"system\",system_message),\n",
    "                                        (\"human\",\"{input}\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d05bbc6a-7ceb-46b3-9489-83f1d8e022dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model=Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "533b673a-1c4a-4c99-8a76-d20e3ad2870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "stuff_doc_chain=create_stuff_documents_chain(model,prompt) #creating stuff document chain or basically chaining llm and prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d641b2bb-1665-44d0-a85d-377a6178b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_fireworks import FireworksEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_storage=Chroma.from_documents(split,OllamaEmbeddings(model=\"nomic-embed-text\"), persist_directory=\"vector_store\",collection_name=\"qna_embeddings\") #initializing our vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c8458fa-9bbb-4bb5-9145-97af54bd7a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x264df65b7f0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fbb63636-a242-43c1-9ae1-8867f95b6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector_storage.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "912749eb-6fec-464d-8d68-9947763181be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever_chain=create_retrieval_chain(retriever,stuff_doc_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b189e2e6-7cec-4c8f-bc63-2f87e8febd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retriever_chain.invoke({\"input\":\"What are some milestone model architectures and papers in the last few years?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc70d6ca-9595-4a58-b58c-452c6cedf498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context provided, I\\'ll answer your question about milestone model architectures and papers in the last few years.\\n\\nIn recent years, several breakthroughs have been made in neural language modeling. Here are a few milestones:\\n\\n1. **Recurrent Neural Networks (RNNs)**: Introduced by Sepp Hochreiter et al. in their 1997 paper \"Long Short-Term Memory,\" RNNs allowed the conditional distribution of a token to depend on the entire context.\\n\\n2. **Transformers**: In 2017, Ashish Vaswani et al. introduced the Transformer architecture for machine translation, which revolutionized sequence-to-sequence tasks and set the stage for further advancements in language modeling.\\n\\n3. **BERT (Bidirectional Encoder Representations from Transformers)**: Released by Jacob Devlin et al. in 2018, BERT is a pre-trained language model that has achieved state-of-the-art results on a wide range of natural language processing (NLP) tasks.\\n\\n4. **GPT-3 (Generative Pre-training of Transformers-3)**: In 2020, Tom Bingham et al. released GPT-3, which represents the culmination of advancements in neural language modeling and has achieved remarkable performance on various NLP tasks.\\n\\n5. **Language Models with Large Context Windows**: Papers like \"Exploring the Limits of Language Modeling\" (2016) by R. Józefowicz, Oriol Vinyals, M. Schuster, Noam M. Shazeer, and others have shown that using larger context windows can lead to improved language modeling performance.\\n\\nThese milestones represent significant advancements in neural language modeling, demonstrating the power of deep learning for processing sequential data like text.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c81b3c0-6ce1-4666-8a4f-d38e793df6b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93dd57ed-6732-4777-9086-31d5e6d00494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stanford-cs324.github.io/winter2022/lectures/introduction/\n",
      "https://stanford-cs324.github.io/winter2022/lectures/introduction/\n",
      "https://stanford-cs324.github.io/winter2022/lectures/introduction/\n",
      "https://stanford-cs324.github.io/winter2022/lectures/introduction/\n"
     ]
    }
   ],
   "source": [
    "for docs in response[\"context\"]:\n",
    "    print(docs.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a8d8395-61c7-4fb4-8b88-e99478050475",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2=retriever_chain.invoke({\"input\":\"what are the types of law?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "615c32cf-9651-4b3e-89de-8e15d815f9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, there are three main types of law mentioned:\\n\\n1. Common law (judiciary): Also known as case law, common law is based on judges referencing previous similar cases and making a ruling (precedent).\\n2. Statutory law (legislature): Also known as written law, statutory law is produced by government agencies through the legislative process (e.g., congress passing a bill).\\n3. Regulatory law (executive): Also known as administrative law, this is law that is created by the executive branch of government, often focusing on procedures.\\n\\nThese types are mentioned in the context to provide an overview of different sources and forms of law.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47b49a04-f170-439f-bf03-090b2dc4ab49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stanford-cs324.github.io/winter2022/lectures/legality/\n",
      "https://stanford-cs324.github.io/winter2022/lectures/legality/\n",
      "https://stanford-cs324.github.io/winter2022/lectures/legality/\n",
      "https://stanford-cs324.github.io/winter2022/lectures/legality/\n"
     ]
    }
   ],
   "source": [
    "for docs in response2[\"context\"]:\n",
    "    print(docs.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f5072-23fc-4511-b3d2-0302eaffff2f",
   "metadata": {},
   "source": [
    "# Putting everything at one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f6a8f-0fad-48dd-afb9-31f997e262bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordLectureBot:\n",
    "    def __init__(self):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
