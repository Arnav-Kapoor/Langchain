{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f27bc086-f299-480c-b50e-0065763dd9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=getpass.getpass()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"FIREWORKS_API_KEY\"]=getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f106b59d-1c6f-4788-ae5d-981118211b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_fireworks import ChatFireworks\n",
    "model=ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bdf4d-8925-4a6c-b072-98901f95aa56",
   "metadata": {},
   "source": [
    "# Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfde08e-1ab8-4b05-89b4-6d9308686efb",
   "metadata": {},
   "source": [
    "## Text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beac095e-7166-425d-bdf2-050a9b9da70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader_text=TextLoader(\"speech.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c501b644-56c1-4203-a1cc-400770c8df16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I have a dream that one day down in Alabama, with its vicious racists, with its governor having his lips dripping with the words of interposition and nullification â€“ one day right there in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers.\\n\\nI have a dream today.\\n\\nI have a dream that one day every valley shall be exalted, and every hill and mountain shall be made low, the rough places will be made plain, and the crooked places will be made straight, and the glory of the Lord shall be revealed and all flesh shall see it together.\\n\\nThis is our hope. This is the faith that I go back to the South with. With this faith we will be able to hew out of the mountain of despair a stone of hope. With this faith we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood. With this faith we will be able to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom together, knowing that we will be free one day.\\n\\nThis will be the day, this will be the day when all of Godâ€™s children will be able to sing with new meaning â€œMy country â€™tis of thee, sweet land of liberty, of thee I sing. Land where my fatherâ€™s died, land of the Pilgrimâ€™s pride, from every mountainside, let freedom ring!', metadata={'source': 'speech.txt'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_docucment=loader_text.load()\n",
    "text_docucment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cef07e-b229-49d0-b0f3-18e2264864c6",
   "metadata": {},
   "source": [
    "## Web based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f95f7f8-695e-4282-be5a-c2ea95f449a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "776979c7-3ae3-4d20-912b-85ca5d4da312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32261a43-c740-49eb-95f1-aea204860efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_web=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                        bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "698ce369-056b-458b-9777-5bedfa76e88e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "web_document=loader_web.load()\n",
    "# web_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca0b9d-0488-4bb8-b614-fe63f3fee4df",
   "metadata": {},
   "source": [
    "## PDF based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bc2b70f-1a08-449f-b007-eb70eda0bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c22749f-a450-414b-b77e-cafbab0affac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc27bd6f-0a14-426e-b40d-3ed5a2775bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_pdf=PyPDFLoader(\"attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83531295-b3ac-4351-967a-ea12d95fe81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_document=loader_pdf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c314be33-fb29-4219-9f16-072619cf242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74523d3e-35a7-4a6a-9b2e-984654b0e914",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a911b298-303c-465b-9276-554eedaa570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=splitter.split_documents(pdf_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4153a4ba-8112-4cd7-bf64-9120e4efd866",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'attention.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9277ba-923a-4cf8-ac26-f2d7829e493b",
   "metadata": {},
   "source": [
    "# Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a0a8304-6aad-4fdf-977e-ee6521ed347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54fd7fe8-d6cc-48e8-a299-1186b34acde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcd29d76-b0c9-4a4a-8e30-c1c2a79ce6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_fireworks import FireworksEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "db=Chroma.from_documents(documents[:20],FireworksEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "081e8c99-df40-41bb-a0b7-ec63d373e964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vector database\n",
    "query=\"What is attention is all you need research paper?\"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde7533-2e83-4935-b5ae-c4e4842908bd",
   "metadata": {},
   "source": [
    "# Retriver and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91d3c1ce-3e93-495a-8797-063d042ba71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_template(\"\"\" Answer the following qustions based only on the context provided.\n",
    "                                            Think step by step before ansering the question.\n",
    "                                            <context>\n",
    "                                            {context}\n",
    "                                            </context>\n",
    "                                            Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3e4fb13-a93a-4b85-bfdc-7061b801c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create stuff document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(model,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62a985d6-e9e6-4e9c-aa11-f9247774ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89e0588d-07c3-48e3-ac9d-55cb936d1ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'FireworksEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x0000016F17D07A60>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3bc7c32c-2154-4523-a68e-4954d4cf7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "94e2ad78-b6e6-4a89-a841-cb48e8724cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"what is attention function\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6ba330f3-630b-4997-9d34-c96ba497b223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, an attention function is a type of mapping that takes a query vector and a set of key-value pairs, all of which are vectors, and produces an output vector. The keys and values are packed together into matrices K and V, respectively. The output is computed as a weighted sum of the values, where the weights are determined by the compatibility of the query with the keys, as given by the softmax function applied to the product of the query and keys matrices, scaled by the square root of the dimension of the keys, dk. The two most commonly used attention functions are dot-product (multiplicative) attention and additive attention. Dot-product attention is identical to the algorithm described in the context, except for a scaling factor of 1/√dk, while additive attention computes the compatibility function using a feed-forward network with a single hidden layer. Dot-product attention is faster and more space-efficient in practice, but additive attention may outperform it for larger values of dk.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996397a-2ea6-4e6b-983b-6025930dc65e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
